{"id":"00000000-0000-0000-0000-000000000000","revision":0,"last_node_id":74,"last_link_id":31,"nodes":[{"id":1,"type":"CLIPTextEncode","pos":[2650,1590],"size":[285.6000061035156,130],"flags":{},"order":62,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":1},{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[7]}],"title":"CLIP Text Encode (Negative Prompt)","properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"CLIPTextEncode"},"widgets_values":["色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走"],"color":"#322","bgcolor":"#533"},{"id":2,"type":"Note","pos":[1840,1390],"size":[400,88],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[],"properties":{},"widgets_values":["Kijai's Clip Vision model can only be loaded by Load WanVideo Clip Encoder Node.\nComfyOrg's Clip Vision model can only be loaded by default Loader."],"color":"#432","bgcolor":"#653"},{"id":6,"type":"Note","pos":[1680,360],"size":[330,130],"flags":{},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"Block Swap","properties":{},"widgets_values":["like --normalvram | --lowvram argument, BlockSwap adjusts the blocks to swap with RAM based on your VRAM, this is a tradeoff between speed and memory usage. \nMax transformer blocks: 40 for 14B, 30 for 1.3B\n\nDo not to exceed the total VRAM, or it will be much more slower because ComfyUI will start unloading models from VRAM to RAM."],"color":"#432","bgcolor":"#653"},{"id":7,"type":"Note","pos":[1680,540],"size":[330,88],"flags":{},"order":2,"mode":0,"inputs":[],"outputs":[],"title":"VRAM Management","properties":{},"widgets_values":["Alternatively there's option to use VRAM management introduced in DiffSynt-Studios. This is usually slower, but saves even more VRAM compared to BlockSwap"],"color":"#432","bgcolor":"#653"},{"id":8,"type":"Note","pos":[2910,160],"size":[310,88],"flags":{},"order":3,"mode":0,"inputs":[],"outputs":[],"title":"Text Encode","properties":{},"widgets_values":["If you've already loaded the diffusion model to the offload device, you don't need to link model_to_offload, which will reduce some loading time."],"color":"#432","bgcolor":"#653"},{"id":12,"type":"WanVideoVAELoader","pos":[1840,940],"size":[400,82],"flags":{},"order":4,"mode":0,"inputs":[{"localized_name":"compile_args","name":"compile_args","shape":7,"type":"WANCOMPILEARGS","link":null},{"localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":null},{"localized_name":"precision","name":"precision","shape":7,"type":"COMBO","widget":{"name":"precision"},"link":null}],"outputs":[{"localized_name":"vae","name":"vae","type":"WANVAE","slot_index":0,"links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoVAELoader"},"widgets_values":["WanVideo\\Wan2_1_VAE_fp32.safetensors","fp32"]},{"id":13,"type":"VAELoader","pos":[1840,830],"size":[400,60],"flags":{},"order":5,"mode":0,"inputs":[{"localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":null}],"outputs":[{"localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"VAELoader","models":[{"name":"wan_2.1_vae.safetensors","url":"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true","directory":"vae"}]},"widgets_values":["WanVideo\\Wan2_1_VAE_fp32.safetensors"]},{"id":18,"type":"SkipLayerGuidanceSD3","pos":[590,1210],"size":[290,130],"flags":{},"order":6,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"layers","name":"layers","type":"STRING","widget":{"name":"layers"},"link":null},{"localized_name":"scale","name":"scale","type":"FLOAT","widget":{"name":"scale"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"SkipLayerGuidanceSD3"},"widgets_values":["7, 8, 9",3,0.01,0.15]},{"id":19,"type":"TorchCompileModel","pos":[-100,1050],"size":[310,60],"flags":{},"order":7,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"backend","name":"backend","type":"COMBO","widget":{"name":"backend"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"TorchCompileModel"},"widgets_values":["inductor"]},{"id":24,"type":"MarkdownNote","pos":[860,1690],"size":[360,510],"flags":{},"order":8,"mode":0,"inputs":[],"outputs":[],"title":"Running Time Comparison (flf2vid)","properties":{},"widgets_values":["HDD & 64G RAM & 3090ti 24G VRAM  \n81 frames at 480x480 with 25 steps  \nSageAtt+TeaCache+SLG+EnhanceAVid  \nwan2.1_i2v_480p_14B_fp8_e4m3fn.safetensors (ComfyOrg)  \n20 blocks swap for Wrapper  \n\n| 1st Run  | Native | Wrapper |\n|----------|--------|---------|\n| Loading  | 760s   | 770s    |\n| Sampling | 420s   | 365s    |\n| Decoding | 10s    | 15s     |\n| Total    | 1190s  | 1150s   |\n\n| 2nd Run  | Native | Wrapper |\n|----------|--------|---------|\n| Loading  | 0s     | 5s      |\n| Sampling | 420s   | 365s    |\n| Decoding | 5s     | 10s     |\n| Total    | 425s   | 380s    |\n\n (sdpa attention in Wrapper workflow needs to reload the diffusion model)  \n| Speedup(without Triton)          | Native | Wrapper    |\n|----------------------------------|--------|------------|\n| SageAtt+TeaCache+SLG+EnhanceAVid | 425s   | 380s       |\n| SageAtt+TeaCache+SLG             | 310s   | 365s       |\n| SageAtt+TeaCache                 | 320s   | 370s       |\n| SageAtt                          | 524s   | 585s       |\n| -                                | 755s   | 825s(sdpa) |"],"color":"#432","bgcolor":"#653"},{"id":25,"type":"MarkdownNote","pos":[490,1690],"size":[360,510],"flags":{},"order":9,"mode":0,"inputs":[],"outputs":[],"title":"Running Time Comparison (img2vid)","properties":{},"widgets_values":["HDD & 64G RAM & 3090ti 24G VRAM  \n81 frames at 480x480 with 25 steps  \nSageAtt+TeaCache+SLG+EnhanceAVid  \nwan2.1_i2v_480p_14B_fp8_e4m3fn.safetensors (ComfyOrg)  \n20 blocks swap for Wrapper  \n\n| 1st Run  | Native | Wrapper |\n|----------|--------|---------|\n| Loading  | 760s   | 770s    |\n| Sampling | 400s   | 345s    |\n| Decoding | 10s    | 15s     |\n| Total    | 1170s  | 1130s   |\n\n| 2nd Run  | Native | Wrapper |\n|----------|--------|---------|\n| Loading  | 0s     | 5s      |\n| Sampling | 400s   | 345s    |\n| Decoding | 5s     | 10s     |\n| Total    | 405s   | 360s    |\n\n (sdpa attention in Wrapper workflow needs to reload the diffusion model)  \n| Speedup(without Triton)          | Native | Wrapper    |\n|----------------------------------|--------|------------|\n| SageAtt+TeaCache+SLG+EnhanceAVid | 405s   | 360s       |\n| SageAtt+TeaCache+SLG             | 295s   | 345s       |\n| SageAtt+TeaCache                 | 305s   | 355s       |\n| SageAtt                          | 500s   | 560s       |\n| -                                | 720s   | 790s(sdpa) |"],"color":"#432","bgcolor":"#653"},{"id":30,"type":"WanVideoBlockSwap","pos":[2040,360],"size":[320,202],"flags":{},"order":10,"mode":0,"inputs":[{"localized_name":"blocks_to_swap","name":"blocks_to_swap","type":"INT","widget":{"name":"blocks_to_swap"},"link":null},{"localized_name":"offload_img_emb","name":"offload_img_emb","type":"BOOLEAN","widget":{"name":"offload_img_emb"},"link":null},{"localized_name":"offload_txt_emb","name":"offload_txt_emb","type":"BOOLEAN","widget":{"name":"offload_txt_emb"},"link":null},{"localized_name":"use_non_blocking","name":"use_non_blocking","shape":7,"type":"BOOLEAN","widget":{"name":"use_non_blocking"},"link":null},{"localized_name":"vace_blocks_to_swap","name":"vace_blocks_to_swap","shape":7,"type":"INT","widget":{"name":"vace_blocks_to_swap"},"link":null},{"localized_name":"prefetch_blocks","name":"prefetch_blocks","shape":7,"type":"INT","widget":{"name":"prefetch_blocks"},"link":null},{"localized_name":"block_swap_debug","name":"block_swap_debug","shape":7,"type":"BOOLEAN","widget":{"name":"block_swap_debug"},"link":null}],"outputs":[{"localized_name":"block_swap_args","name":"block_swap_args","type":"BLOCKSWAPARGS","slot_index":0,"links":[31]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoBlockSwap"},"widgets_values":[20,false,false,true,0,0,false],"color":"#223","bgcolor":"#335"},{"id":32,"type":"SkipLayerGuidanceDiT","pos":[590,980],"size":[290,180],"flags":{},"order":11,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"double_layers","name":"double_layers","type":"STRING","widget":{"name":"double_layers"},"link":null},{"localized_name":"single_layers","name":"single_layers","type":"STRING","widget":{"name":"single_layers"},"link":null},{"localized_name":"scale","name":"scale","type":"FLOAT","widget":{"name":"scale"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null},{"localized_name":"rescaling_scale","name":"rescaling_scale","type":"FLOAT","widget":{"name":"rescaling_scale"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"SkipLayerGuidanceDiT"},"widgets_values":["7, 8, 9","7, 8, 9",3,0.01,0.15,0]},{"id":39,"type":"UNETLoader","pos":[2270,2030],"size":[450,82],"flags":{},"order":12,"mode":0,"inputs":[{"localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"},"link":null},{"localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"UNETLoader","models":[{"name":"wan2.1_i2v_480p_14B_fp16.safetensors","url":"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true","directory":"diffusion_models"}]},"widgets_values":["WanVideo\\ComfyOrg\\wan2.1_i2v_480p_14B_fp8_e4m3fn.safetensors","fp8_e4m3fn"]},{"id":40,"type":"WanVideoModelLoader","pos":[2760,2030],"size":[450,338],"flags":{},"order":13,"mode":0,"inputs":[{"localized_name":"compile_args","name":"compile_args","shape":7,"type":"WANCOMPILEARGS","link":null},{"localized_name":"block_swap_args","name":"block_swap_args","shape":7,"type":"BLOCKSWAPARGS","link":null},{"localized_name":"lora","name":"lora","shape":7,"type":"WANVIDLORA","link":null},{"localized_name":"vram_management_args","name":"vram_management_args","shape":7,"type":"VRAM_MANAGEMENTARGS","link":null},{"localized_name":"extra_model","name":"extra_model","shape":7,"type":"VACEPATH","link":null},{"localized_name":"fantasytalking_model","name":"fantasytalking_model","shape":7,"type":"FANTASYTALKINGMODEL","link":null},{"localized_name":"multitalk_model","name":"multitalk_model","shape":7,"type":"MULTITALKMODEL","link":null},{"localized_name":"fantasyportrait_model","name":"fantasyportrait_model","shape":7,"type":"FANTASYPORTRAITMODEL","link":null},{"localized_name":"model","name":"model","type":"COMBO","widget":{"name":"model"},"link":null},{"localized_name":"base_precision","name":"base_precision","type":"COMBO","widget":{"name":"base_precision"},"link":null},{"localized_name":"quantization","name":"quantization","type":"COMBO","widget":{"name":"quantization"},"link":null},{"localized_name":"load_device","name":"load_device","type":"COMBO","widget":{"name":"load_device"},"link":null},{"localized_name":"attention_mode","name":"attention_mode","shape":7,"type":"COMBO","widget":{"name":"attention_mode"},"link":null},{"localized_name":"rms_norm_function","name":"rms_norm_function","shape":7,"type":"COMBO","widget":{"name":"rms_norm_function"},"link":null},{"localized_name":"vace_model","name":"vace_model","shape":7,"type":"VACEPATH","link":null}],"outputs":[{"localized_name":"model","name":"model","type":"WANVIDEOMODEL","slot_index":0,"links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoModelLoader"},"widgets_values":["WanVideo\\ComfyOrg\\wan2.1_i2v_480p_14B_fp8_e4m3fn.safetensors","bf16","disabled","offload_device","sageattn","default"]},{"id":42,"type":"MarkdownNote","pos":[770,80],"size":[450,380],"flags":{},"order":14,"mode":0,"inputs":[],"outputs":[],"title":"Speedup and Enhance methods","properties":{},"widgets_values":["Speed UP:  \n[Triton (Torch Compile)](https://github.com/woct0rdho/triton-windows)  \n~30% speed increase if you have Triton installed and enabled.  \n[SageAttention](https://github.com/thu-ml/SageAttention)  \nalmost double inference speed if you have Sageattn installed and enabled.   \n[TeaCache](https://github.com/ali-vilab/TeaCache)  \nTeaCache could be considered to be sort of an automated step skipper.  \nThe relative l1 threshold -value determines how aggressive this is, higher values are faster but quality suffers more. Very first steps should NEVER be skipped with this model or it kills the motion. When using the pre-calculated coefficients, the treshold value should be much higher than with the default coefficients.  \n[Patch Model Patch Order](https://www.reddit.com/r/StableDiffusion/comments/1gjl982/lora_torchcompile_is_now_possible_thanks_to/)  \nWith weight_patch_first, switching to a different LoRA is really fast, no need for full recompile.  \n\n[Triton and SageAttention Installation](https://old.reddit.com/r/StableDiffusion/comments/1h7hunp/how_to_run_hunyuanvideo_on_a_single_24gb_vram_card/)  \n[Triton:supported GPU](https://github.com/woct0rdho/triton-windows#1-gpu)  \n[Triton:fp8 is not supported on RTX 30xx and older GPUs](https://github.com/woct0rdho/triton-windows#fp8-is-not-supported-on-rtx-30xx-and-older-gpus)  \n\nEnhance:  \n[Skip Layer Guidance](https://www.reddit.com/r/StableDiffusion/comments/1jac3wm/dramatically_enhance_the_quality_of_wan_21_using/)  \n[CFG Zero Star](https://github.com/WeichenFan/CFG-Zero-star)  \n[Enhance A Video](https://oahzxl.github.io/Enhance_A_Video/)  \nEnhance-a-video can increase the fidelity of the results (also time), too high values lead to noisy results.  \n[Unet Temporal Attention Multiply](https://www.runcomfy.com/comfyui-nodes/ComfyUI/UNetTemporalAttentionMultiply)  \nuseful for tasks that require temporal coherence and consistency, such as video frame interpolation, temporal segmentation, and other time-series related applications.  \n[RIFLEx](https://github.com/thu-ml/RIFLEx)  \nExtent video."],"color":"#432","bgcolor":"#653"},{"id":46,"type":"Note","pos":[2410,620],"size":[470,88],"flags":{},"order":15,"mode":0,"inputs":[],"outputs":[],"title":"fp8_fast and fp16_fast precision","properties":{},"widgets_values":["fp8_fast seems to cause huge quality degradation\n\nfp_16_fast enables \"Full FP16 Accmumulation in FP16 GEMMs\" feature available in the very latest pytorch nightly, this is around 20% speed boost. "],"color":"#432","bgcolor":"#653"},{"id":47,"type":"Note","pos":[1010,500],"size":[210,90],"flags":{},"order":16,"mode":0,"inputs":[],"outputs":[],"title":"Resolution","properties":{},"widgets_values":["360p : 640x360\n480p : 832x480 (854x480)\n540p : 960x544 (960x540)\n720p : 1280x720\n1080p: 1920x1080"],"color":"#432","bgcolor":"#653"},{"id":50,"type":"Note","pos":[770,630],"size":[230,88],"flags":{},"order":17,"mode":0,"inputs":[],"outputs":[],"title":"First Last Frame img2vid","properties":{},"widgets_values":["1. WanAI Wan i2v\n2. WanAI Wan flf2v\n3. Alibaba pai Wan Fun Inpaint\n4. Alibaba vi lab VACE flf"],"color":"#432","bgcolor":"#653"},{"id":41,"type":"Note","pos":[310,80],"size":[450,250],"flags":{},"order":18,"mode":0,"inputs":[],"outputs":[],"title":"vram arguments","properties":{},"widgets_values":["VRAM>RAM: --highvram\n  By default models will be unloaded to RAM after being used. This option keeps them in GPU memory.\n\nRAM>VRAM: --normalvram (default) or --lowvram\n  Split the unet in parts to use less VRAM.\n  ComfyUI does not unload models from VRAM unless there is insufficient space to load another model, as it aims to maximize the reuse of already loaded models. Without this feature, even when slightly modifying the parameters of a workflow and rerunning it, there will be a cost of reloading into VRAM.\n  This feature can be disabled using the --disable-smart-memory option.\nWhen --disable-smart-memory is applied, all data in VRAM is unloaded to RAM upon the completion of the workflow (based on the --normalvram setting).\n  Choose --normalvram when --lowvram isn't enough VRAM.\n\n--cpu\n  To use the CPU for everything (slow).\n"],"color":"#432","bgcolor":"#653"},{"id":33,"type":"TorchCompileModelWanVideo","pos":[-100,820],"size":[312.3999938964844,180],"flags":{},"order":19,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"backend","name":"backend","type":"COMBO","widget":{"name":"backend"},"link":null},{"localized_name":"fullgraph","name":"fullgraph","type":"BOOLEAN","widget":{"name":"fullgraph"},"link":null},{"localized_name":"mode","name":"mode","type":"COMBO","widget":{"name":"mode"},"link":null},{"localized_name":"dynamic","name":"dynamic","type":"BOOLEAN","widget":{"name":"dynamic"},"link":null},{"localized_name":"dynamo_cache_size_limit","name":"dynamo_cache_size_limit","type":"INT","widget":{"name":"dynamo_cache_size_limit"},"link":null},{"localized_name":"compile_transformer_blocks_only","name":"compile_transformer_blocks_only","type":"BOOLEAN","widget":{"name":"compile_transformer_blocks_only"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[]}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"a5bd3c86c8ed6b83c55c2d0e7a59515b15a0137f","Node name for S&R":"TorchCompileModelWanVideo"},"widgets_values":["inductor",false,"default",false,64,false]},{"id":34,"type":"WanVideoTorchCompileSettings","pos":[-100,1170],"size":[310,202],"flags":{},"order":20,"mode":0,"inputs":[{"localized_name":"backend","name":"backend","type":"COMBO","widget":{"name":"backend"},"link":null},{"localized_name":"fullgraph","name":"fullgraph","type":"BOOLEAN","widget":{"name":"fullgraph"},"link":null},{"localized_name":"mode","name":"mode","type":"COMBO","widget":{"name":"mode"},"link":null},{"localized_name":"dynamic","name":"dynamic","type":"BOOLEAN","widget":{"name":"dynamic"},"link":null},{"localized_name":"dynamo_cache_size_limit","name":"dynamo_cache_size_limit","type":"INT","widget":{"name":"dynamo_cache_size_limit"},"link":null},{"localized_name":"compile_transformer_blocks_only","name":"compile_transformer_blocks_only","type":"BOOLEAN","widget":{"name":"compile_transformer_blocks_only"},"link":null},{"localized_name":"dynamo_recompile_limit","name":"dynamo_recompile_limit","shape":7,"type":"INT","widget":{"name":"dynamo_recompile_limit"},"link":null}],"outputs":[{"localized_name":"torch_compile_args","name":"torch_compile_args","type":"WANCOMPILEARGS","slot_index":0,"links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoTorchCompileSettings"},"widgets_values":["inductor",false,"default",false,64,true,128],"color":"#233","bgcolor":"#355"},{"id":43,"type":"UNetTemporalAttentionMultiply","pos":[-100,1470],"size":[310,130],"flags":{},"order":21,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"self_structural","name":"self_structural","type":"FLOAT","widget":{"name":"self_structural"},"link":null},{"localized_name":"self_temporal","name":"self_temporal","type":"FLOAT","widget":{"name":"self_temporal"},"link":null},{"localized_name":"cross_structural","name":"cross_structural","type":"FLOAT","widget":{"name":"cross_structural"},"link":null},{"localized_name":"cross_temporal","name":"cross_temporal","type":"FLOAT","widget":{"name":"cross_temporal"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"UNetTemporalAttentionMultiply"},"widgets_values":[1,1,1,1]},{"id":26,"type":"WanVideoTeaCache","pos":[240,1030],"size":[315,178],"flags":{},"order":22,"mode":0,"inputs":[{"localized_name":"rel_l1_thresh","name":"rel_l1_thresh","type":"FLOAT","widget":{"name":"rel_l1_thresh"},"link":null},{"localized_name":"start_step","name":"start_step","type":"INT","widget":{"name":"start_step"},"link":null},{"localized_name":"end_step","name":"end_step","type":"INT","widget":{"name":"end_step"},"link":null},{"localized_name":"cache_device","name":"cache_device","type":"COMBO","widget":{"name":"cache_device"},"link":null},{"localized_name":"use_coefficients","name":"use_coefficients","type":"BOOLEAN","widget":{"name":"use_coefficients"},"link":null},{"localized_name":"mode","name":"mode","shape":7,"type":"COMBO","widget":{"name":"mode"},"link":null}],"outputs":[{"localized_name":"cache_args","name":"cache_args","type":"CACHEARGS","links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoTeaCache"},"widgets_values":[0.25,1,-1,"offload_device","true","e"],"color":"#233","bgcolor":"#355"},{"id":35,"type":"WanVideoTeaCacheKJ","pos":[240,820],"size":[310,154],"flags":{},"order":23,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"rel_l1_thresh","name":"rel_l1_thresh","type":"FLOAT","widget":{"name":"rel_l1_thresh"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null},{"localized_name":"cache_device","name":"cache_device","type":"COMBO","widget":{"name":"cache_device"},"link":null},{"localized_name":"coefficients","name":"coefficients","type":"COMBO","widget":{"name":"coefficients"},"link":null}],"outputs":[{"localized_name":"model","name":"model","type":"MODEL","slot_index":0,"links":[]}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"68db110554d5f1d9bef8d027a111a49fd7f85e1b","Node name for S&R":"WanVideoTeaCacheKJ"},"widgets_values":[0.25000000000000006,0.1,1,"offload_device","14B"]},{"id":53,"type":"SkipLayerGuidanceWanVideo","pos":[590,820],"size":[290,110],"flags":{},"order":24,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"blocks","name":"blocks","type":"STRING","widget":{"name":"blocks"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[]}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"8ecf5cd05e0a1012087b0da90eea9a13674668db","Node name for S&R":"SkipLayerGuidanceWanVideo"},"widgets_values":["10",0.2,1]},{"id":20,"type":"WanVideoSLG","pos":[590,1390],"size":[290,106],"flags":{},"order":25,"mode":0,"inputs":[{"localized_name":"blocks","name":"blocks","type":"STRING","widget":{"name":"blocks"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null}],"outputs":[{"localized_name":"slg_args","name":"slg_args","type":"SLGARGS","links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"cc8450b1a7f57d5a8ab25ba3dd040847056a95f4","Node name for S&R":"WanVideoSLG"},"widgets_values":["10",0.20000000000000004,1],"color":"#233","bgcolor":"#355"},{"id":52,"type":"WanVideoEnhanceAVideoKJ","pos":[920,820],"size":[300,80],"flags":{},"order":26,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"latent","name":"latent","type":"LATENT","link":null},{"localized_name":"weight","name":"weight","type":"FLOAT","widget":{"name":"weight"},"link":null}],"outputs":[{"localized_name":"model","name":"model","type":"MODEL","links":[]}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"8ecf5cd05e0a1012087b0da90eea9a13674668db","Node name for S&R":"WanVideoEnhanceAVideoKJ"},"widgets_values":[0.2]},{"id":22,"type":"WanVideoEnhanceAVideo","pos":[920,960],"size":[300,106],"flags":{},"order":27,"mode":0,"inputs":[{"localized_name":"weight","name":"weight","type":"FLOAT","widget":{"name":"weight"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null}],"outputs":[{"localized_name":"feta_args","name":"feta_args","type":"FETAARGS","links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoEnhanceAVideo"},"widgets_values":[2,0,1],"color":"#233","bgcolor":"#355"},{"id":21,"type":"CFGZeroStar","pos":[920,1170],"size":[300,26],"flags":{},"order":28,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null}],"outputs":[{"localized_name":"patched_model","name":"patched_model","type":"MODEL","links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"CFGZeroStar"},"widgets_values":[]},{"id":28,"type":"WanVideoExperimentalArgs","pos":[920,1260],"size":[304,370],"flags":{},"order":29,"mode":0,"inputs":[{"localized_name":"video_attention_split_steps","name":"video_attention_split_steps","type":"STRING","widget":{"name":"video_attention_split_steps"},"link":null},{"localized_name":"cfg_zero_star","name":"cfg_zero_star","type":"BOOLEAN","widget":{"name":"cfg_zero_star"},"link":null},{"localized_name":"use_zero_init","name":"use_zero_init","type":"BOOLEAN","widget":{"name":"use_zero_init"},"link":null},{"localized_name":"zero_star_steps","name":"zero_star_steps","type":"INT","widget":{"name":"zero_star_steps"},"link":null},{"localized_name":"use_fresca","name":"use_fresca","type":"BOOLEAN","widget":{"name":"use_fresca"},"link":null},{"localized_name":"fresca_scale_low","name":"fresca_scale_low","type":"FLOAT","widget":{"name":"fresca_scale_low"},"link":null},{"localized_name":"fresca_scale_high","name":"fresca_scale_high","type":"FLOAT","widget":{"name":"fresca_scale_high"},"link":null},{"localized_name":"fresca_freq_cutoff","name":"fresca_freq_cutoff","type":"INT","widget":{"name":"fresca_freq_cutoff"},"link":null},{"localized_name":"use_tcfg","name":"use_tcfg","type":"BOOLEAN","widget":{"name":"use_tcfg"},"link":null},{"localized_name":"raag_alpha","name":"raag_alpha","type":"FLOAT","widget":{"name":"raag_alpha"},"link":null},{"localized_name":"bidirectional_sampling","name":"bidirectional_sampling","type":"BOOLEAN","widget":{"name":"bidirectional_sampling"},"link":null},{"localized_name":"temporal_score_rescaling","name":"temporal_score_rescaling","type":"BOOLEAN","widget":{"name":"temporal_score_rescaling"},"link":null},{"localized_name":"tsr_k","name":"tsr_k","type":"FLOAT","widget":{"name":"tsr_k"},"link":null},{"localized_name":"tsr_sigma","name":"tsr_sigma","type":"FLOAT","widget":{"name":"tsr_sigma"},"link":null}],"outputs":[{"localized_name":"exp_args","name":"exp_args","type":"EXPERIMENTALARGS","links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoExperimentalArgs"},"widgets_values":["",true,false,0,false,1,1.25,20,false,0,false,false,0.95,1],"color":"#233","bgcolor":"#355"},{"id":51,"type":"MarkdownNote","pos":[120,1690],"size":[360,510],"flags":{},"order":30,"mode":0,"inputs":[],"outputs":[],"title":"Running Time Comparison (txt2Vid)","properties":{},"widgets_values":["HDD & 64G RAM & 3090ti 24G VRAM  \n81 frames at 480x480 with 25 steps  \nSageAtt+TeaCache+SLG+EnhanceAVid  \nwan2.1_t2v_14B_fp8_e4m3fn.safetensors (ComfyOrg)  \n19 blocks swap for Wrapper  \n\n| 1st Run  | Native | Wrapper |\n|----------|--------|---------|\n| Loading  | 935s?  | 750s    |\n| Sampling | 435s   | 380s    |\n| Decoding | 10s    | 20s     |\n| Total    | 1380s  | 1150s   |\n\n| 2nd Run  | Native | Wrapper |\n|----------|--------|---------|\n| Loading  | 0s     | 5s      |\n| Sampling | 435s   | 380s    |\n| Decoding | 5s     | 10s     |\n| Total    | 440s   | 395s    |\n\n (sdpa attention in Wrapper workflow needs to reload the diffusion model)  \n| Speedup(without Triton)          | Native | Wrapper    |\n|----------------------------------|--------|------------|\n| SageAtt+TeaCache+SLG+EnhanceAVid | 440s   | 395s       |\n| SageAtt+TeaCache+SLG             | 300s   | 380s       |\n| SageAtt+TeaCache                 | 315s   | 395s       |\n| SageAtt                          | 520s   | 590s       |\n| -                                | 745s   | 835s(sdpa) |"],"color":"#432","bgcolor":"#653"},{"id":11,"type":"CLIPVisionLoader","pos":[1840,1130],"size":[400,58],"flags":{},"order":31,"mode":0,"inputs":[{"localized_name":"clip_name","name":"clip_name","type":"COMBO","widget":{"name":"clip_name"},"link":null}],"outputs":[{"localized_name":"CLIP_VISION","name":"CLIP_VISION","type":"CLIP_VISION","links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.26","Node name for S&R":"CLIPVisionLoader"},"widgets_values":["WanVideo\\clip_vision_h.safetensors"]},{"id":3,"type":"WanVideoTorchCompileSettings","pos":[1930,100],"size":[430,202],"flags":{},"order":32,"mode":0,"inputs":[{"localized_name":"backend","name":"backend","type":"COMBO","widget":{"name":"backend"},"link":null},{"localized_name":"fullgraph","name":"fullgraph","type":"BOOLEAN","widget":{"name":"fullgraph"},"link":null},{"localized_name":"mode","name":"mode","type":"COMBO","widget":{"name":"mode"},"link":null},{"localized_name":"dynamic","name":"dynamic","type":"BOOLEAN","widget":{"name":"dynamic"},"link":null},{"localized_name":"dynamo_cache_size_limit","name":"dynamo_cache_size_limit","type":"INT","widget":{"name":"dynamo_cache_size_limit"},"link":null},{"localized_name":"compile_transformer_blocks_only","name":"compile_transformer_blocks_only","type":"BOOLEAN","widget":{"name":"compile_transformer_blocks_only"},"link":null},{"localized_name":"dynamo_recompile_limit","name":"dynamo_recompile_limit","shape":7,"type":"INT","widget":{"name":"dynamo_recompile_limit"},"link":null}],"outputs":[{"localized_name":"torch_compile_args","name":"torch_compile_args","type":"WANCOMPILEARGS","slot_index":0,"links":[30]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoTorchCompileSettings"},"widgets_values":["inductor",false,"default",false,64,true,128],"color":"#223","bgcolor":"#335"},{"id":36,"type":"Note","pos":[1340,50],"size":[320,660],"flags":{},"order":33,"mode":0,"inputs":[],"outputs":[],"title":"VRAM settings","properties":{},"widgets_values":["- >=12G VRAM, t2v 1.3B f32 model, f32 precision,\n  disabled quantization, main device\n  \n- 8G VRAM, t2v 1.3B f32 model, f32 precision,\n  disabled quantization, offload device\n  recomemdded blocks to swap: 10-12\n  \n  10 (might slightly exceed VRAM)\n  Transformer blocks on cpu: 1771.57MB\n  Transformer blocks on cuda:0: 3543.14MB\n  Total memory used by transformer blocks: 5314.72MB\n  Max allocated memory: max_memory=5.708 GB\n  Max reserved memory: max_reserved=6.219 GB\n  \n  11 (might just use up)\n  Transformer blocks on cpu: 1948.73MB\n  Transformer blocks on cuda:0: 3365.99MB\n  Total memory used by transformer blocks: 5314.72MB\n  Max allocated memory: max_memory=5.535 GB\n  Max reserved memory: max_reserved=6.062 GB\n\n  12 (remians extra VRAM)\n  Transformer blocks on cpu: 2125.89MB\n  Transformer blocks on cuda:0: 3188.83MB\n  Total memory used by transformer blocks: 5314.72MB\n  Max allocated memory: max_memory=5.362 GB\n  Max reserved memory: max_reserved=5.906 GB\n\n- 24G VRAM, i2v 14B f8 model, bf16 precision,\n  disabled quantization, offload device\n  recomemdded blocks to swap: 18-20\n  \n  18 (might slightly exceed VRAM)\n  Transformer blocks on cpu: 13865.75MB\n  Transformer blocks on cuda:0: 16947.03MB\n  Total memory used by transformer blocks: 30812.77MB\n  Max allocated memory: max_memory=21.781 GB\n  Max reserved memory: max_reserved=22.344 GB\n  \n  19 (might just use up)\n  Transformer blocks on cpu: 14636.07MB\n  Transformer blocks on cuda:0: 16176.71MB\n  Total memory used by transformer blocks: 30812.77MB\n  Max allocated memory: max_memory=21.028 GB\n  Max reserved memory: max_reserved=21.531 GB\n\n  20 (remians extra VRAM)\n  Transformer blocks on cpu: 15406.39MB\n  Transformer blocks on cuda:0: 15406.39MB\n  Total memory used by transformer blocks: 30812.77MB\n  Max allocated memory: max_memory=20.276 GB\n  Max reserved memory: max_reserved=20.844 GB"],"color":"#432","bgcolor":"#653"},{"id":29,"type":"Note","pos":[2410,70],"size":[470,190],"flags":{},"order":34,"mode":0,"inputs":[],"outputs":[],"title":"Diffusion Model Loader","properties":{},"widgets_values":["Quantization converts the diffusion model on the load which will reduce the VRAM and loading time even more. \n\nDuring sampling the weights are cast to base_precision when doing the actual calculations.\n\nmain_device: load transformer parameters to cuda (much quicker)\noffload_device: load transfomer paramters to cpu\n\nIn most cases: main_device for 1.3B fp32 model, offload_device for 14B fp8 model.\n\nOnly real reason to use main_device would be on cloud GPUs with more VRAM than RAM where you can do that to avoid swapping the models between GPU/CPU."],"color":"#432","bgcolor":"#653"},{"id":23,"type":"PatchModelPatcherOrder","pos":[250,1380],"size":[310,82],"flags":{},"order":35,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"patch_order","name":"patch_order","type":"COMBO","widget":{"name":"patch_order"},"link":null},{"localized_name":"full_load","name":"full_load","type":"COMBO","widget":{"name":"full_load"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[]}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"8ecf5cd05e0a1012087b0da90eea9a13674668db","Node name for S&R":"PatchModelPatcherOrder"},"widgets_values":["weight_patch_first","auto"]},{"id":27,"type":"PathchSageAttentionKJ","pos":[250,1270],"size":[310,58],"flags":{},"order":36,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"sage_attention","name":"sage_attention","type":"COMBO","widget":{"name":"sage_attention"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[]}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"9a15e22f5e9416c0968ce3de33923f8f601257dd","Node name for S&R":"PathchSageAttentionKJ"},"widgets_values":["auto"]},{"id":55,"type":"ApplyRifleXRoPE_WanVideo","pos":[250,1520],"size":[315,78],"flags":{},"order":37,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":null},{"localized_name":"latent","name":"latent","type":"LATENT","link":null},{"localized_name":"k","name":"k","type":"INT","widget":{"name":"k"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":null}],"properties":{"cnr_id":"comfyui-kjnodes","ver":"b7e5b6f1e2b7c79b3f1e4b4bfe5e1687715803ab","Node name for S&R":"ApplyRifleXRoPE_WanVideo"},"widgets_values":[6]},{"id":10,"type":"Note","pos":[310,370],"size":[450,300],"flags":{"collapsed":false},"order":38,"mode":0,"inputs":[],"outputs":[],"title":"Quantization","properties":{},"widgets_values":["Quality rank (highest to lowest):\n  fp32 > fp16 > bf16 > fp8_scaled > fp8_e4m3 | fp8_e5m2\n\n  fp vs. bf (https://medium.com/@furkangozukara/what-is-the-difference-between-fp16-and-bf16-here-a-good-explanation-for-you-d75ac7ec30fa)\n\nFP = Floating Point. Any signed floating point number is stored as 3 parts:\n  Sign bit, Mantissa, Exponent\n  number = sign * mantissa * 2 ^ exponent\n\nE5M2 means that 2 bits represent mantissa and 5 bits represent exponent.\nE4M3 means that 3 bits represent mantissa and 4 bits represent exponent.\n\nE5M2 can represent wider range of numbers than E4M3 at cost of lower precision of the numbers. But the amount of different numbers that can be represented are the same: 256 distinct values. So if we need more precision around 0 then we use E4M3 and if we need more precision closer to min/max values then we use E5M2.\n\nSo actually the best way to choose what format to use would be to analyse distribution of weight values in the model. And if they tend to be closer to zero we use E4M3 or E5M2 otherwise."],"color":"#432","bgcolor":"#653"},{"id":44,"type":"Note","pos":[1330,790],"size":[470,550],"flags":{},"order":39,"mode":0,"inputs":[],"outputs":[],"title":"Model","properties":{},"widgets_values":["Models:\nhttps://huggingface.co/Wan-AI\nhttps://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files\nhttps://huggingface.co/Kijai/WanVideo_comfy/tree/main\n\n1. VAE:\n   different loader share model, but cannot replace each other\n   ① wan_2.1_VAE.safetensors (ComfyOrg)\n   ② Wan2_1_VAE_fp32.safetensors (Kijai) (recomended)\n     Wan2_1_VAE_bf16.safetensors (Kijai)\n\n2. Clip Vision:\n   different loader doesn't share model, but can replace each other\n   ① clip_vision_h.safetensors (ComfyOrg) (recomended)\n   ② open-clip-xlm-roberta-large-vit-huge-14_visual_fp32.safetensors (Kijai)\n     open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors (Kijai)\n\n3. Clip(Text Encoder):\n   different loader doesn't share model, and cannot replace each other\n   You can use WanVideo TextEmbed Bridge Node to integrate the native Text Encoder process in the Wrapper workflow.\n   ① umt5_xxl_fp16.safetensors (ComfyOrg)\n     umt5_xxl_fp8_e4m3fn_scaled.safetensors (ComfyOrg)\n   ② umt5-xxl-enc-bf16.safetensors (Kijai)\n     umt5-xxl-enc-fp8_e4m3fn.safetensors (Kijai)\n   \n4. Diffusion(variation):\n   different loader share model, but cannot replace each other\n   For my 64G RAM & 3090ti 24G VRAM:\n   Bypass Triton and choose f8 series from ComfyOrg | Kijai.\n   If you choose bf16 series from ComfyOrg, enable Triton, bypass TeaCacheKJ and SLGWanVideoKJ (incompatable issue) in Native workflow.\n\n1.3B Control Lora:\nhttps://huggingface.co/spacepxl/Wan2.1-control-loras/tree/main/1.3b/depth\nhttps://huggingface.co/spacepxl/Wan2.1-control-loras/tree/main/1.3b/tile\n\nFun Control:\nhttps://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control/blob/main\nhttps://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/tree/main\nFun Inpaint:\nhttps://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-InP/blob/main\nhttps://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/tree/main"],"color":"#432","bgcolor":"#653"},{"id":49,"type":"Note","pos":[1010,630],"size":[210,88],"flags":{},"order":40,"mode":0,"inputs":[],"outputs":[],"title":"Reference(subject)2vid","properties":{},"widgets_values":["1. Alibaba vi lab VACE\n2. Phantom-video Phantam\n3. SkyworkAI Skyreel A2"],"color":"#432","bgcolor":"#653"},{"id":16,"type":"WanVideoTextEncode","pos":[2910,300],"size":[310,234],"flags":{},"order":41,"mode":0,"inputs":[{"localized_name":"t5","name":"t5","shape":7,"type":"WANTEXTENCODER","link":null},{"localized_name":"model_to_offload","name":"model_to_offload","shape":7,"type":"WANVIDEOMODEL","link":null},{"localized_name":"positive_prompt","name":"positive_prompt","type":"STRING","widget":{"name":"positive_prompt"},"link":null},{"localized_name":"negative_prompt","name":"negative_prompt","type":"STRING","widget":{"name":"negative_prompt"},"link":null},{"localized_name":"force_offload","name":"force_offload","shape":7,"type":"BOOLEAN","widget":{"name":"force_offload"},"link":null},{"localized_name":"use_disk_cache","name":"use_disk_cache","shape":7,"type":"BOOLEAN","widget":{"name":"use_disk_cache"},"link":null},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","slot_index":0,"links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoTextEncode"},"widgets_values":["a monkey robot is smiling","色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走",true,false,"gpu"],"color":"#332922","bgcolor":"#593930"},{"id":17,"type":"LoadWanVideoT5TextEncoder","pos":[2280,880],"size":[460,130],"flags":{},"order":42,"mode":0,"inputs":[{"localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":null},{"localized_name":"precision","name":"precision","type":"COMBO","widget":{"name":"precision"},"link":null},{"localized_name":"load_device","name":"load_device","shape":7,"type":"COMBO","widget":{"name":"load_device"},"link":null},{"localized_name":"quantization","name":"quantization","shape":7,"type":"COMBO","widget":{"name":"quantization"},"link":null}],"outputs":[{"localized_name":"wan_t5_model","name":"wan_t5_model","type":"WANTEXTENCODER","slot_index":0,"links":[9,16]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"LoadWanVideoT5TextEncoder"},"widgets_values":["WanVideo\\Kijai\\umt5-xxl-enc-fp8_e4m3fn.safetensors","bf16","offload_device","disabled"]},{"id":38,"type":"CLIPLoader","pos":[2280,1450],"size":[320,106],"flags":{},"order":43,"mode":0,"inputs":[{"localized_name":"clip_name","name":"clip_name","type":"COMBO","widget":{"name":"clip_name"},"link":null},{"localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"},"link":null},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[1,5,18]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"CLIPLoader","models":[{"name":"umt5_xxl_fp8_e4m3fn_scaled.safetensors","url":"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true","directory":"text_encoders"}]},"widgets_values":["WanVideo\\ComfyOrg\\umt5_xxl_fp8_e4m3fn_scaled.safetensors","wan","default"]},{"id":14,"type":"CLIPTextEncode","pos":[2650,1450],"size":[285.6000061035156,100],"flags":{},"order":63,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":5},{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[6]}],"title":"CLIP Text Encode (Positive Prompt)","properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"CLIPTextEncode"},"widgets_values":["a monkey robot is smiling"],"color":"#232","bgcolor":"#353"},{"id":45,"type":"LoadWanVideoClipTextEncoder","pos":[1840,1240],"size":[400,106],"flags":{},"order":44,"mode":0,"inputs":[{"localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":null},{"localized_name":"precision","name":"precision","type":"COMBO","widget":{"name":"precision"},"link":null},{"localized_name":"load_device","name":"load_device","shape":7,"type":"COMBO","widget":{"name":"load_device"},"link":null}],"outputs":[{"localized_name":"wan_clip_vision","name":"wan_clip_vision","type":"CLIP_VISION","slot_index":0,"links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"LoadWanVideoClipTextEncoder"},"widgets_values":["WanVideo\\Kijai\\open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors","bf16","offload_device"]},{"id":65,"type":"WanVideoApplyNAG","pos":[4380,850],"size":[300,126],"flags":{},"order":69,"mode":0,"inputs":[{"localized_name":"original_text_embeds","name":"original_text_embeds","type":"WANVIDEOTEXTEMBEDS","link":22},{"localized_name":"nag_text_embeds","name":"nag_text_embeds","type":"WANVIDEOTEXTEMBEDS","link":23},{"localized_name":"nag_scale","name":"nag_scale","type":"FLOAT","widget":{"name":"nag_scale"},"link":null},{"localized_name":"nag_tau","name":"nag_tau","type":"FLOAT","widget":{"name":"nag_tau"},"link":null},{"localized_name":"nag_alpha","name":"nag_alpha","type":"FLOAT","widget":{"name":"nag_alpha"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"907d1b0b0f649bd569bbf23430e107ab37832d73","Node name for S&R":"WanVideoApplyNAG"},"widgets_values":[11,2.5,0.25]},{"id":67,"type":"WanVideoContextOptions","pos":[-500,1130],"size":[280,202],"flags":{},"order":45,"mode":0,"inputs":[{"localized_name":"reference_latent","name":"reference_latent","shape":7,"type":"LATENT","link":null},{"localized_name":"context_schedule","name":"context_schedule","type":"COMBO","widget":{"name":"context_schedule"},"link":null},{"localized_name":"context_frames","name":"context_frames","type":"INT","widget":{"name":"context_frames"},"link":null},{"localized_name":"context_stride","name":"context_stride","type":"INT","widget":{"name":"context_stride"},"link":null},{"localized_name":"context_overlap","name":"context_overlap","type":"INT","widget":{"name":"context_overlap"},"link":null},{"localized_name":"freenoise","name":"freenoise","type":"BOOLEAN","widget":{"name":"freenoise"},"link":null},{"localized_name":"verbose","name":"verbose","type":"BOOLEAN","widget":{"name":"verbose"},"link":null},{"localized_name":"fuse_method","name":"fuse_method","shape":7,"type":"COMBO","widget":{"name":"fuse_method"},"link":null}],"outputs":[{"localized_name":"context_options","name":"context_options","type":"WANVIDCONTEXT","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"4f9ef5b7a96729545ffb828b16e01b954c20c597","Node name for S&R":"WanVideoContextOptions"},"widgets_values":["uniform_standard",81,4,16,true,false,"linear"]},{"id":68,"type":"WanVideoLoopArgs","pos":[-500,1380],"size":[280,106],"flags":{},"order":46,"mode":0,"inputs":[{"localized_name":"shift_skip","name":"shift_skip","type":"INT","widget":{"name":"shift_skip"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null}],"outputs":[{"localized_name":"loop_args","name":"loop_args","type":"LOOPARGS","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"4f9ef5b7a96729545ffb828b16e01b954c20c597","Node name for S&R":"WanVideoLoopArgs"},"widgets_values":[6,0,1]},{"id":69,"type":"WanVideoFreeInitArgs","pos":[-500,1540],"size":[280,154],"flags":{},"order":47,"mode":0,"inputs":[{"localized_name":"freeinit_num_iters","name":"freeinit_num_iters","type":"INT","widget":{"name":"freeinit_num_iters"},"link":null},{"localized_name":"freeinit_method","name":"freeinit_method","type":"COMBO","widget":{"name":"freeinit_method"},"link":null},{"localized_name":"freeinit_n","name":"freeinit_n","type":"INT","widget":{"name":"freeinit_n"},"link":null},{"localized_name":"freeinit_d_s","name":"freeinit_d_s","type":"FLOAT","widget":{"name":"freeinit_d_s"},"link":null},{"localized_name":"freeinit_d_t","name":"freeinit_d_t","type":"FLOAT","widget":{"name":"freeinit_d_t"},"link":null}],"outputs":[{"localized_name":"freeinit_args","name":"freeinit_args","type":"FREEINITARGS","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"4f9ef5b7a96729545ffb828b16e01b954c20c597","Node name for S&R":"WanVideoFreeInitArgs"},"widgets_values":[3,"ideal",4,1,1]},{"id":72,"type":"MarkdownNote","pos":[-860,1540],"size":[330,130],"flags":{},"order":48,"mode":0,"inputs":[],"outputs":[],"title":"Free Init","properties":{},"widgets_values":["Free Init is a long-established training-free stabilization technique developed specially for text2video models. It allows iterative refinement of videos to have better movement consistency and prompt following. Although it multiplies the rendering time, with new methods such as causvid, it is not now very slowing. And a good thing is that it doesn't increase VRAM consumption.  \n[Kijai's 2-sampler method](https://github.com/kijai/ComfyUI-WanVideoWrapper/pull/685):  \n2nd sampler using Free Init."],"color":"#432","bgcolor":"#653"},{"id":71,"type":"MarkdownNote","pos":[-860,1130],"size":[330,130],"flags":{},"order":49,"mode":0,"inputs":[],"outputs":[],"title":"Long Context","properties":{},"widgets_values":["Long context option is only for txt2vid? Maybe not.  \n[Tea Cache issues](https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/160#issuecomment-2706873273).  \nYou can write very similar prompts and separate them with '|' to use multiple prompts with it.  \nFor uniform_standard schedule, morphing artifacts are inevitable. uniform_looped can achieve looped video might be better.  \n\nwith skyreel, maybe this node can be ignored."],"color":"#432","bgcolor":"#653"},{"id":70,"type":"Note","pos":[-860,1380],"size":[330,88],"flags":{},"order":50,"mode":0,"inputs":[],"outputs":[],"title":"Loop","properties":{},"widgets_values":["Loop is only for txt2vid and doesn't like TeaCache.\nThe skip count should be half of the latent count, so the 6 is for 13 latents (49 frames).\nshift_skip = (num_frames - 1) / 8"],"color":"#432","bgcolor":"#653"},{"id":57,"type":"WanVideoApplyNAG","pos":[3280,880],"size":[300,126],"flags":{},"order":66,"mode":0,"inputs":[{"localized_name":"original_text_embeds","name":"original_text_embeds","type":"WANVIDEOTEXTEMBEDS","link":26},{"localized_name":"nag_text_embeds","name":"nag_text_embeds","type":"WANVIDEOTEXTEMBEDS","link":27},{"localized_name":"nag_scale","name":"nag_scale","type":"FLOAT","widget":{"name":"nag_scale"},"link":null},{"localized_name":"nag_tau","name":"nag_tau","type":"FLOAT","widget":{"name":"nag_tau"},"link":null},{"localized_name":"nag_alpha","name":"nag_alpha","type":"FLOAT","widget":{"name":"nag_alpha"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"907d1b0b0f649bd569bbf23430e107ab37832d73","Node name for S&R":"WanVideoApplyNAG"},"widgets_values":[11,2.5,0.25]},{"id":61,"type":"Note","pos":[3280,1060],"size":[300,240],"flags":{},"order":51,"mode":0,"inputs":[],"outputs":[],"properties":{},"widgets_values":["nag_scale: The scale for attention feature extrapolation. Higher values result in stronger negative guidance.\nnag_tau: The normalisation threshold. Higher values result in stronger negative guidance.\nnag_alpha: Blending factor between original and extrapolated attention. Higher values result in stronger negative guidance.\n\nFor image-reference tasks (e.g., Image2Video), use lower nag_tau and nag_alpha to preserve the reference content more faithfully.\nFor models that require more sampling steps and higher CFG, also prefer lower nag_tau and nag_alpha.\nFor few-step models, you can use higher nag_tau and nag_alpha to have stronger negative guidance."],"color":"#432","bgcolor":"#653"},{"id":59,"type":"CLIPTextEncode","pos":[2650,1800],"size":[303.6000061035156,130],"flags":{},"order":64,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":18},{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[20]}],"title":"CLIP Text Encode (NAG Negative Prompt)","properties":{"cnr_id":"comfy-core","ver":"0.3.29","Node name for S&R":"CLIPTextEncode"},"widgets_values":["色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走"],"color":"#322","bgcolor":"#533"},{"id":63,"type":"WanVideoTextEncodeCached","pos":[3980,850],"size":[350,302],"flags":{},"order":65,"mode":0,"inputs":[{"localized_name":"extender_args","name":"extender_args","shape":7,"type":"WANVIDEOPROMPTEXTENDER_ARGS","link":21},{"localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":null},{"localized_name":"precision","name":"precision","type":"COMBO","widget":{"name":"precision"},"link":null},{"localized_name":"positive_prompt","name":"positive_prompt","type":"STRING","widget":{"name":"positive_prompt"},"link":null},{"localized_name":"negative_prompt","name":"negative_prompt","type":"STRING","widget":{"name":"negative_prompt"},"link":null},{"localized_name":"quantization","name":"quantization","type":"COMBO","widget":{"name":"quantization"},"link":null},{"localized_name":"use_disk_cache","name":"use_disk_cache","type":"BOOLEAN","widget":{"name":"use_disk_cache"},"link":null},{"localized_name":"device","name":"device","type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":[22]},{"localized_name":"negative_text_embeds","name":"negative_text_embeds","type":"WANVIDEOTEXTEMBEDS","links":[23]},{"localized_name":"positive_prompt","name":"positive_prompt","type":"STRING","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"69dd4689bfd69d37d7f997289cb980e45f8b19bb","Node name for S&R":"WanVideoTextEncodeCached"},"widgets_values":["WanVideo\\ComfyOrg\\umt5_xxl_fp8_e4m3fn_scaled.safetensors","bf16","","","disabled",true,"gpu"]},{"id":64,"type":"WanVideoPromptExtenderSelect","pos":[3630,850],"size":[300,154],"flags":{},"order":52,"mode":0,"inputs":[{"localized_name":"custom_system_prompt","name":"custom_system_prompt","shape":7,"type":"STRING","link":null},{"localized_name":"model","name":"model","type":"COMBO","widget":{"name":"model"},"link":null},{"localized_name":"max_new_tokens","name":"max_new_tokens","type":"INT","widget":{"name":"max_new_tokens"},"link":null},{"localized_name":"system_prompt","name":"system_prompt","type":"COMBO","widget":{"name":"system_prompt"},"link":null},{"localized_name":"seed","name":"seed","shape":7,"type":"INT","widget":{"name":"seed"},"link":null}],"outputs":[{"localized_name":"extender_args","name":"extender_args","type":"WANVIDEOPROMPTEXTENDER_ARGS","links":[21]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"69dd4689bfd69d37d7f997289cb980e45f8b19bb","Node name for S&R":"WanVideoPromptExtenderSelect"},"widgets_values":["QWenImage\\qwen_2.5_vl_7b_fp8_scaled.safetensors",512,"T2V Movie Director (Chinese)",0,"randomize"]},{"id":66,"type":"WanVideoExperimentalArgs","pos":[-500,780],"size":[280,370],"flags":{},"order":53,"mode":0,"inputs":[{"localized_name":"video_attention_split_steps","name":"video_attention_split_steps","type":"STRING","widget":{"name":"video_attention_split_steps"},"link":null},{"localized_name":"cfg_zero_star","name":"cfg_zero_star","type":"BOOLEAN","widget":{"name":"cfg_zero_star"},"link":null},{"localized_name":"use_zero_init","name":"use_zero_init","type":"BOOLEAN","widget":{"name":"use_zero_init"},"link":null},{"localized_name":"zero_star_steps","name":"zero_star_steps","type":"INT","widget":{"name":"zero_star_steps"},"link":null},{"localized_name":"use_fresca","name":"use_fresca","type":"BOOLEAN","widget":{"name":"use_fresca"},"link":null},{"localized_name":"fresca_scale_low","name":"fresca_scale_low","type":"FLOAT","widget":{"name":"fresca_scale_low"},"link":null},{"localized_name":"fresca_scale_high","name":"fresca_scale_high","type":"FLOAT","widget":{"name":"fresca_scale_high"},"link":null},{"localized_name":"fresca_freq_cutoff","name":"fresca_freq_cutoff","type":"INT","widget":{"name":"fresca_freq_cutoff"},"link":null},{"localized_name":"use_tcfg","name":"use_tcfg","type":"BOOLEAN","widget":{"name":"use_tcfg"},"link":null},{"localized_name":"raag_alpha","name":"raag_alpha","type":"FLOAT","widget":{"name":"raag_alpha"},"link":null},{"localized_name":"bidirectional_sampling","name":"bidirectional_sampling","type":"BOOLEAN","widget":{"name":"bidirectional_sampling"},"link":null},{"localized_name":"temporal_score_rescaling","name":"temporal_score_rescaling","type":"BOOLEAN","widget":{"name":"temporal_score_rescaling"},"link":null},{"localized_name":"tsr_k","name":"tsr_k","type":"FLOAT","widget":{"name":"tsr_k"},"link":null},{"localized_name":"tsr_sigma","name":"tsr_sigma","type":"FLOAT","widget":{"name":"tsr_sigma"},"link":null}],"outputs":[{"localized_name":"exp_args","name":"exp_args","type":"EXPERIMENTALARGS","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"4f9ef5b7a96729545ffb828b16e01b954c20c597","Node name for S&R":"WanVideoExperimentalArgs"},"widgets_values":["",false,false,0,false,1,1.25,20,false,0,false,false,0.95,1]},{"id":37,"type":"WanVideoTextEncode","pos":[2770,880],"size":[450,234],"flags":{},"order":60,"mode":0,"inputs":[{"localized_name":"t5","name":"t5","shape":7,"type":"WANTEXTENCODER","link":9},{"localized_name":"model_to_offload","name":"model_to_offload","shape":7,"type":"WANVIDEOMODEL","link":null},{"localized_name":"positive_prompt","name":"positive_prompt","type":"STRING","widget":{"name":"positive_prompt"},"link":null},{"localized_name":"negative_prompt","name":"negative_prompt","type":"STRING","widget":{"name":"negative_prompt"},"link":null},{"localized_name":"force_offload","name":"force_offload","shape":7,"type":"BOOLEAN","widget":{"name":"force_offload"},"link":null},{"localized_name":"use_disk_cache","name":"use_disk_cache","shape":7,"type":"BOOLEAN","widget":{"name":"use_disk_cache"},"link":null},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","slot_index":0,"links":[26]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoTextEncode"},"widgets_values":["a monkey robot is smiling","色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走",true,false,"gpu"]},{"id":58,"type":"WanVideoTextEncodeSingle","pos":[2770,1180],"size":[450,180],"flags":{},"order":61,"mode":0,"inputs":[{"localized_name":"t5","name":"t5","shape":7,"type":"WANTEXTENCODER","link":16},{"localized_name":"model_to_offload","name":"model_to_offload","shape":7,"type":"WANVIDEOMODEL","link":null},{"localized_name":"prompt","name":"prompt","type":"STRING","widget":{"name":"prompt"},"link":null},{"localized_name":"force_offload","name":"force_offload","shape":7,"type":"BOOLEAN","widget":{"name":"force_offload"},"link":null},{"localized_name":"use_disk_cache","name":"use_disk_cache","shape":7,"type":"BOOLEAN","widget":{"name":"use_disk_cache"},"link":null},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":[27]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"907d1b0b0f649bd569bbf23430e107ab37832d73","Node name for S&R":"WanVideoTextEncodeSingle"},"widgets_values":["色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走",true,false,"gpu"]},{"id":15,"type":"WanVideoTextEmbedBridge","pos":[2970,1450],"size":[250,46],"flags":{},"order":67,"mode":0,"inputs":[{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":6},{"localized_name":"negative","name":"negative","shape":7,"type":"CONDITIONING","link":7}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":[28]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"18aa47cc74534241835d2682bc5b51293082529c","Node name for S&R":"WanVideoTextEmbedBridge"},"widgets_values":[]},{"id":62,"type":"WanVideoTextEmbedBridge","pos":[2970,1800],"size":[250,46],"flags":{},"order":68,"mode":0,"inputs":[{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":20},{"localized_name":"negative","name":"negative","shape":7,"type":"CONDITIONING","link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":[29]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"18aa47cc74534241835d2682bc5b51293082529c","Node name for S&R":"WanVideoTextEmbedBridge"},"widgets_values":[]},{"id":73,"type":"WanVideoApplyNAG","pos":[3280,1450],"size":[300,126],"flags":{},"order":70,"mode":0,"inputs":[{"localized_name":"original_text_embeds","name":"original_text_embeds","type":"WANVIDEOTEXTEMBEDS","link":28},{"localized_name":"nag_text_embeds","name":"nag_text_embeds","type":"WANVIDEOTEXTEMBEDS","link":29},{"localized_name":"nag_scale","name":"nag_scale","type":"FLOAT","widget":{"name":"nag_scale"},"link":null},{"localized_name":"nag_tau","name":"nag_tau","type":"FLOAT","widget":{"name":"nag_tau"},"link":null},{"localized_name":"nag_alpha","name":"nag_alpha","type":"FLOAT","widget":{"name":"nag_alpha"},"link":null}],"outputs":[{"localized_name":"text_embeds","name":"text_embeds","type":"WANVIDEOTEXTEMBEDS","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"907d1b0b0f649bd569bbf23430e107ab37832d73","Node name for S&R":"WanVideoApplyNAG"},"widgets_values":[11,2.5,0.25]},{"id":48,"type":"MarkdownNote","pos":[-100,1690],"size":[210,340],"flags":{},"order":54,"mode":0,"inputs":[],"outputs":[],"properties":{},"widgets_values":["Recommend native workflow:  \n1. Native Video Inpaint  \n2. Native VACE Video Repaint\n3. Depth | Tile Lora (error occur when wrapper loading model)\n4. Not using features below:\n- Skyreel [V2 DF](https://github.com/SkyworkAI/SkyReels-V2)\n- [ReCamMaster](https://github.com/KwaiVGI/ReCamMaster)\n- [MiniMax Remover](https://github.com/zibojia/MiniMax-Remover)\n- [Fantasy Talking](https://github.com/Fantasy-AMAP/fantasy-talking)\n- [Multi Talking](https://github.com/MeiGen-AI/MultiTalk)\n- [Fantasy Portrait](https://github.com/Fantasy-AMAP/fantasy-portrait)\n- [Unianimate](https://github.com/ali-vilab/UniAnimate)\n- [Stand-In](https://www.stand-in.tech/)\n- [Uni3C](https://github.com/ewrfcas/Uni3C)\n- [Flow Edit](https://github.com/fallenshock/FlowEdit)\n- [Dilated ControlNet](https://github.com/TheDenk/wan2.1-dilated-controlnet)\n- [Radial Attention](https://github.com/mit-han-lab/radial-attention)\n- [Enhanced A Video](https://oahzxl.github.io/Enhance_A_Video/)\n- [FreeInit](https://tianxingwu.github.io/pages/FreeInit/)\n- [RAAG](https://arxiv.org/abs/2508.03442)\n- [Bidirectional Sampling](https://github.com/ff2416/WanFM)\n- [Loop](https://github.com/YisuiTT/Mobius/) "],"color":"#432","bgcolor":"#653"},{"id":74,"type":"WanVideoModelLoader","pos":[2410,320],"size":[470,318],"flags":{},"order":59,"mode":0,"inputs":[{"localized_name":"compile_args","name":"compile_args","shape":7,"type":"WANCOMPILEARGS","link":30},{"localized_name":"block_swap_args","name":"block_swap_args","shape":7,"type":"BLOCKSWAPARGS","link":31},{"localized_name":"lora","name":"lora","shape":7,"type":"WANVIDLORA","link":null},{"localized_name":"vram_management_args","name":"vram_management_args","shape":7,"type":"VRAM_MANAGEMENTARGS","link":null},{"localized_name":"extra_model","name":"extra_model","shape":7,"type":"VACEPATH","link":null},{"localized_name":"fantasytalking_model","name":"fantasytalking_model","shape":7,"type":"FANTASYTALKINGMODEL","link":null},{"localized_name":"multitalk_model","name":"multitalk_model","shape":7,"type":"MULTITALKMODEL","link":null},{"localized_name":"fantasyportrait_model","name":"fantasyportrait_model","shape":7,"type":"FANTASYPORTRAITMODEL","link":null},{"localized_name":"model","name":"model","type":"COMBO","widget":{"name":"model"},"link":null},{"localized_name":"base_precision","name":"base_precision","type":"COMBO","widget":{"name":"base_precision"},"link":null},{"localized_name":"quantization","name":"quantization","type":"COMBO","widget":{"name":"quantization"},"link":null},{"localized_name":"load_device","name":"load_device","type":"COMBO","widget":{"name":"load_device"},"link":null},{"localized_name":"attention_mode","name":"attention_mode","shape":7,"type":"COMBO","widget":{"name":"attention_mode"},"link":null},{"localized_name":"rms_norm_function","name":"rms_norm_function","shape":7,"type":"COMBO","widget":{"name":"rms_norm_function"},"link":null}],"outputs":[{"localized_name":"model","name":"model","type":"WANVIDEOMODEL","links":null}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"c57ffd8150ba60e4656bca12bd1c876892dff5cd","Node name for S&R":"WanVideoModelLoader"},"widgets_values":["WanVideo\\_Main\\ComfyOrg\\wan2.1_t2v_14B_fp8_e4m3fn.safetensors","bf16","disabled","offload_device","sdpa","default"]},{"id":4,"type":"WanVideoVRAMManagement","pos":[2040,610],"size":[320,58],"flags":{},"order":55,"mode":0,"inputs":[{"localized_name":"offload_percent","name":"offload_percent","type":"FLOAT","widget":{"name":"offload_percent"},"link":null}],"outputs":[{"localized_name":"vram_management_args","name":"vram_management_args","type":"VRAM_MANAGEMENTARGS","links":[]}],"properties":{"cnr_id":"ComfyUI-WanVideoWrapper","ver":"d9b1f4d1a5aea91d101ae97a54714a5861af3f50","Node name for S&R":"WanVideoVRAMManagement"},"widgets_values":[1],"color":"#223","bgcolor":"#335"},{"id":31,"type":"Note","pos":[770,500],"size":[230,88],"flags":{},"order":56,"mode":0,"inputs":[],"outputs":[],"title":"Triton error","properties":{},"widgets_values":["type fp8e4nv (fp8 scaled | fp8_e4m3fn) is not supported in CUDA arch < 89 (rtx 20xx or 30xx).\nThe supported fp8 dtypes are ('fp8e4b15', 'fp8e5')?"],"color":"#432","bgcolor":"#653"},{"id":5,"type":"Note","pos":[1680,100],"size":[230,200],"flags":{},"order":57,"mode":0,"inputs":[],"outputs":[],"title":"Triton","properties":{},"widgets_values":["Wan Video Torch Compile Settings is disconnected by default and will not take effect if you're using fp8 e4m3 diffusion model and your graphic card is 20xx or 30xx.\n\nIf you choose vace fp32/bf16/fp16/fp8 e5m2 model, then you can:\n1. Link Wan Video Torch Compile Settings to the Wan Video Model Loader to speed up about 15%~25%.\n2. Follow the VRAM settings instruction in the Wan Video Wrapper Tips Group."],"color":"#432","bgcolor":"#653"},{"id":56,"type":"MarkdownNote","pos":[1330,1550],"size":[580,1900],"flags":{},"order":58,"mode":0,"inputs":[],"outputs":[],"title":"WAN eco","properties":{},"widgets_values":["| Features | Function | Usage |\n|----------|----------|-------|\n| ComfyOrg [2.2](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged)  [2.1](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main) | | |\n| Kijai [WAN](https://huggingface.co/Kijai/WanVideo_comfy/tree/main)  [WAN fp8 scaled](https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled) | | |\n| *Common Feature* | | |\n| [Triton (Torch Compile)](https://github.com/woct0rdho/triton-windows) | speed up | ~ |\n| [Radial Attention](https://github.com/mit-han-lab/radial-attention) | speed up | ~ |\n| [Sage Attention](https://github.com/thu-ml/SageAttention) | speed up | ~ |\n| [Flash Attention](https://github.com/Dao-AILab/flash-attention) | speed up | ~ |\n| [SDP Attention](https://uxlfoundation.github.io/oneDNN/dev_guide_graph_sdpa.html) | speed up | ~ |\n| [EasyCache](https://github.com/H-EmbodVis/EasyCache) | speed up | ~ |\n| [MagCache](https://github.com/Zehong-Ma/MagCache) | speed up | ~ |\n| [TeaCache](https://github.com/ali-vilab/TeaCache) | speed up | ~ |\n| [Enhanced A Video](https://oahzxl.github.io/Enhance_A_Video/) | enhancement | ~ |\n| [SLG](https://www.reddit.com/r/StableDiffusion/comments/1jac3wm/dramatically_enhance_the_quality_of_wan_21_using/) | enhancement | ~ |\n| [CFG Zero Star](https://github.com/WeichenFan/CFG-Zero-star) | enhancement | ~ |\n| [FreSca](https://github.com/WikiChao/FreSca) | enhancement | ~ |\n| [FreeInit](https://tianxingwu.github.io/pages/FreeInit/) | enhancement | ~ |\n| [NAG](https://chendaryen.github.io/NAG.github.io/) | enhancement | ~ |\n| [TCFG](https://huggingface.co/papers/2503.18137) | enhancement | ~ |\n| [RAAG](https://arxiv.org/abs/2508.03442) | enhancement | ~ |\n| [Bidirectional Sampling](https://github.com/ff2416/WanFM) | enhancement | ~ |\n| [TSR](https://github.com/temporalscorerescaling/TSR) | enhancement | ~ |\n| [Loop](https://github.com/YisuiTT/Mobius/) | loop video | ~ |\n| [RIFLEx](https://github.com/thu-ml/RIFLEx) | long video | ~ |\n| [Context Window](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved) | long & loop video | ~ |\n| [Flow Edit](https://github.com/fallenshock/FlowEdit) | in content edit | v2v |\n| *Main Model* | | |\n| WAN Video [2.2](https://github.com/Wan-Video/Wan2.2) [2.1](https://github.com/Wan-Video/Wan2.1) | main model | t2v ff2v flf2v v2v s2v |\n| WAN Video GGUF [2.2](https://huggingface.co/collections/QuantStack/wan22-ggufs-6887ec891bdea453a35b95f3) [2.1](https://huggingface.co/city96) | main model | t2v ff2v flf2v v2v |\n| VACE [2.2 Test](https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test) [2.2 Fake](https://huggingface.co/CCP6/FakeVace2.2) [2.1](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) | main & module model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE GGUF [2.1](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF) [2.1](https://github.com/ali-vilab/VACE) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE LightX2V [2.1](https://huggingface.co/lym00/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE LightX2V GGUF [2.1](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE FusionX [2.1](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_FusionX_VACE) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE FusionX GGUF [2.1](https://huggingface.co/collections/QuantStack/wan21-fusionx-ggufs-68498e41b3597737512c0636) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE Phantom [2.1](https://huggingface.co/Inner-Reflections/Wan2.1_VACE_Phantom) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| VACE SF (GGUF) [2.1](https://huggingface.co/lym00) | main model | t2v ff2v flf2v ref2v v2v ref+v2v |\n| *Low-Step model* | | |\n| lightX2V [2.2](https://huggingface.co/lightx2v/Wan2.2-Lightning) [2.1](https://huggingface.co/lightx2v) | main model & lora | t2v ff2v v2v lcm simple steps 4 cfg 1.0 |\n| FusionX [2.1](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX) | main model & lora | t2v ff2v v2v uni_pc simple steps 8 cfg 1.0|\n| FusionX GGUF [2.1](https://huggingface.co/collections/QuantStack/wan21-fusionx-ggufs-68498e41b3597737512c0636) | main model | t2v ff2v v2v uni_pc simple steps 8 cfg 1.0|\n| CausVid [2.1](https://github.com/tianweiy/CausVid) | main model & lora | t2v ff2v v2v uni_pc(flowmatch_causvid) simple steps 8 cfg 1.0 |\n| AccVid [2.1](https://github.com/aejion/AccVideo) | main model & lora | t2v ff2v v2v uni_pc simple steps 8 cfg 1.0 |\n| Fast Wan [2.2](https://huggingface.co/FastVideo) [2.1](https://huggingface.co/FastVideo) | main model & lora | t2v v2v uni_pc simple steps 8 cfg 1.0 |\n| Turbo [2.2 TI2V 5B](https://github.com/quanhaol/Wan2.2-TI2V-5B-Turbo) | main model & lora | t2v v2v uni_pc simple steps 4 cfg 1.0 |\n| Pusa* [2.2](https://github.com/Yaofang-Liu/Pusa-VidGen) [2.1](https://github.com/Yaofang-Liu/Pusa-VidGen) | lora | t2v ff2v flf2v v2v uni_pc(flowmatch_pusa) simple steps 5 cfg 5.0 |\n| *Functional Model* | | |\n| FUN Control [2.2](https://huggingface.co/collections/alibaba-pai/wan22-fun-68958eabec343b948f1225c5) [2.1](https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17) | main model & lora | t2v ff2v |\n| FUN InP [2.2](https://huggingface.co/collections/alibaba-pai/wan22-fun-68958eabec343b948f1225c5) [2.1](https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17) | main model & lora | ff2v flf2v |\n| FUN InP Reward LoRA [2.2](https://huggingface.co/alibaba-pai/Wan2.2-Fun-Reward-LoRAs) [2.1](https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs) | lora | t2v ff2v flf2v v2v |\n| FUN Camera [2.2](https://huggingface.co/collections/alibaba-pai/wan22-fun-68958eabec343b948f1225c5) [2.1](https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17) | main model | ff2v |\n| FUN VACE [2.2](https://huggingface.co/alibaba-pai/Wan2.2-VACE-Fun-A14B) | main & module model | t2v ff2v ref2v |\n| HuMo [2.1](https://phantom-video.github.io/HuMo) | main model | t2v ff2v ref2v v2v s2v |\n| Phantom [2.1](https://github.com/Phantom-video/Phantom) | main model & lora | ref2v ref+v2v |\n| Stand-In [2.1](https://www.stand-in.tech/) | lora | ref2v ref+v2v |\n| MAGREF [2.1](https://github.com/MAGREF-Video/MAGREF) | main model | ref2v |\n| Skyreel A2 [2.1](https://github.com/SkyworkAI/SkyReels-A2) | main model | ref2v |\n| Skyreel V2 [2.1](https://github.com/SkyworkAI/SkyReels-V2) | main & lora | t2v ff2v v2v |\n| Skyreel V2 DF [2.1](https://github.com/SkyworkAI/SkyReels-V2) | main DF model | t2v ff2v v2v |\n| MoviiGen [2.1](https://huggingface.co/ZuluVision/MoviiGen1.1) | main model & lora | t2v v2v |\n| CineScale [2.1](https://github.com/Eyeline-Labs/CineScale) | lora | t2v ff2v | \n| lynx [2.1](https://github.com/bytedance/lynx) | lora | t2v | \n| Echo Shot [2.1](https://github.com/JoHnneyWang/EchoShot) | main model & lora | t2v |\n| ATI [2.1](https://github.com/bytedance/ATI) | main model | ff2v |\n| AniSora [2.2](https://huggingface.co/IndexTeam/Index-anisora/tree/main/V3.2) [2.1](https://github.com/bilibili/Index-anisora) | main model | ff2v |\n| ReCamMaster [2.1](https://github.com/KwaiVGI/ReCamMaster) | main model | v2v |\n| Lumen [2.1](https://lumen-relight.github.io/) | main model & lora | v2v |\n| MiniMax Remover [2.1](https://github.com/zibojia/MiniMax-Remover) | main model | v2v |\n| Fantasy Talking [2.1](https://github.com/Fantasy-AMAP/fantasy-talking) | module model | ff2v v2v |\n| Multi Talk [2.1](https://github.com/MeiGen-AI/MultiTalk) | module model | ff2v v2v |\n| Infinite Talk [2.1](https://github.com/MeiGen-AI/InfiniteTalk) | module model | ff2v v2v |\n| Ovi [2.2](https://github.com/character-ai/Ovi) | main model | t2av i2av |\n| Fantasy Portrait [2.1](https://github.com/Fantasy-AMAP/fantasy-portrait) | module model | ff2v v2v |\n| MTV Crafter [2.1](https://github.com/DINGYANB/MTVCrafter) | main & module model | t2v ff2v |\n| *control* | | |\n| Unianimate [2.1](https://github.com/ali-vilab/UniAnimate) | control lora | ff2v |\n| Depth lora [2.1](https://huggingface.co/spacepxl/Wan2.1-control-loras/tree/main/1.3b/depth) | control lora | t2v |\n| Tile lora [2.1](https://huggingface.co/spacepxl/Wan2.1-control-loras/tree/main/1.3b/tile) | control lora | v2v |\n| Dilated ControlNet [2.2](https://huggingface.co/collections/TheDenk/wan22-controlnets-688b754ca3ee3bc7b34253bf) [2.1](https://github.com/TheDenk/wan2.1-dilated-controlnet) | controlnet | t2v v2v |\n| Uni3C* [2.1](https://github.com/ewrfcas/Uni3C) | 3D controlnet | ff2v |\n| RealisDance* [2.1](https://github.com/damo-cv/RealisDance) | 3D controlnet | ff2v |"],"color":"#432","bgcolor":"#653"}],"links":[[1,38,0,1,0,"CLIP"],[5,38,0,14,0,"CLIP"],[6,14,0,15,0,"CONDITIONING"],[7,1,0,15,1,"CONDITIONING"],[9,17,0,37,0,"WANTEXTENCODER"],[16,17,0,58,0,"WANTEXTENCODER"],[18,38,0,59,0,"CLIP"],[20,59,0,62,0,"CONDITIONING"],[21,64,0,63,0,"WANVIDEOPROMPTEXTENDER_ARGS"],[22,63,0,65,0,"WANVIDEOTEXTEMBEDS"],[23,63,1,65,1,"WANVIDEOTEXTEMBEDS"],[26,37,0,57,0,"WANVIDEOTEXTEMBEDS"],[27,58,0,57,1,"WANVIDEOTEXTEMBEDS"],[28,15,0,73,0,"WANVIDEOTEXTEMBEDS"],[29,62,0,73,1,"WANVIDEOTEXTEMBEDS"],[30,3,0,74,0,"WANCOMPILEARGS"],[31,30,0,74,1,"BLOCKSWAPARGS"]],"groups":[{"id":1,"title":"Wan Video Wrapper Tips","bounding":[1330,-20,1900,743.5999755859375],"color":"#3f789e","font_size":24,"flags":{}},{"id":2,"title":"You might need to know...","bounding":[300,10,930,721.5999755859375],"color":"#3f789e","font_size":24,"flags":{}},{"id":3,"title":"VAE","bounding":[1830,760,420,275.6000061035156],"color":"#3f789e","font_size":24,"flags":{}},{"id":4,"title":"CLIP Vision","bounding":[1830,1060,420,431.6000061035156],"color":"#3f789e","font_size":24,"flags":{}},{"id":5,"title":"Text Encode","bounding":[2260,770,1330,1173.5999755859375],"color":"#3f789e","font_size":24,"flags":{}},{"id":6,"title":"Diffusion","bounding":[2260,1960,960,377.6000061035156],"color":"#3f789e","font_size":24,"flags":{}},{"id":7,"title":"Speedup and Enhance nodes","bounding":[-110,750,1340,860],"color":"#3f789e","font_size":24,"flags":{}},{"id":8,"title":"Running Time Comparison","bounding":[-110,1620,1340,593.5999755859375],"color":"#3f789e","font_size":24,"flags":{}},{"id":9,"title":"basic text embed","bounding":[2270,810,960,317.6000061035156],"color":"#3f789e","font_size":24,"flags":{}},{"id":10,"title":"basic text embed","bounding":[2270,1380,960,353.6000061035156],"color":"#3f789e","font_size":24,"flags":{}},{"id":11,"title":"text encode cached","bounding":[3620,780,1070,385.6000061035156],"color":"#3f789e","font_size":24,"flags":{}}],"config":{},"extra":{"ds":{"scale":1.2560399411848213,"offset":[-464.36382364075365,-1393.7801674858977]}},"version":0.4}